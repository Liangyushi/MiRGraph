{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2de651-c965-4bb9-bfa6-2969b61dd18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ReduceLROnPlateau####\n",
    "import os\n",
    "import pickle\n",
    "#import rpy2.robjects as robjects\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from torchvision import transforms as tfs\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "from torch_geometric.loader import HGTLoader\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch.cuda.amp import autocast\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, precision_recall_curve, average_precision_score\n",
    "#from torchsummary import summary\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class HGT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads, num_layers,node_types,metadata):\n",
    "        super().__init__()\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in node_types:\n",
    "            self.lin_dict[node_type] = Linear(-1, hidden_channels[0])\n",
    "        # self.lin_dict['miRNA']=Linear(2656, hidden_channels[0])\n",
    "        # self.lin_dict['gene']=Linear(18454, hidden_channels[0])\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            conv = HGTConv(hidden_channels[i], hidden_channels[i+1], metadata,num_heads)\n",
    "            self.convs.append(conv)\n",
    "        # self.lin1 = Linear(hidden_channels[-1], out_channels)\n",
    "        # self.lin2 = Linear(hidden_channels[-1], out_channels)\n",
    "        self.relu = nn.GELU()\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for node_type, x in x_dict.items():\n",
    "            x_dict[node_type] = self.lin_dict[node_type](x).relu_()\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "        # xm=self.relu(self.lin1(x_dict['miRNA']))\n",
    "        # xg=self.relu(self.lin2(x_dict['gene']))\n",
    "        xm=x_dict['miRNA']\n",
    "        xg=x_dict['gene']\n",
    "        return (xm,xg)\n",
    "\n",
    "# class MLPBilPredictor(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, num_layers, dropout_rate=0.3):\n",
    "#         super(MLPBilPredictor, self).__init__()\n",
    "#         self.lins = torch.nn.ModuleList()\n",
    "#         self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "#         for _ in range(num_layers - 1):\n",
    "#             self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "#         self.bilin = torch.nn.Linear(hidden_channels, hidden_channels, bias=False)\n",
    "#         self.relu = nn.GELU()\n",
    "#\n",
    "#     def reset_parameters(self):\n",
    "#         for lin in self.lins:\n",
    "#             lin.reset_parameters()\n",
    "#         self.bilin.reset_parameters()\n",
    "#\n",
    "#     def forward(self, x_i, x_j):\n",
    "#         for lin in self.lins:\n",
    "#             x_i, x_j = lin(x_i), lin(x_j)\n",
    "#             x_i, x_j = self.relu(x_i), self.relu(x_j)\n",
    "#         x = torch.sum(self.bilin(x_i) * x_j, dim=-1)\n",
    "#         # x2 = torch.sum(self.bilin(x_j) * x_i, dim=-1)\n",
    "#         # x = x1+x2\n",
    "#         #x = torch.sum(x_i * x_j, dim=-1)\n",
    "#         return x\n",
    "\n",
    "class MLPBilPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, dropout_rate=0.3):\n",
    "        super(MLPBilPredictor, self).__init__()\n",
    "        # self.lins = torch.nn.ModuleList()\n",
    "        # self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        # for _ in range(num_layers - 1):\n",
    "        #     self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.bilin = torch.nn.Linear(hidden_channels, hidden_channels, bias=False)\n",
    "        # self.bilin = torch.nn.Linear(in_channels,in_channels, bias=False)\n",
    "        # self.relu = nn.GELU()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # for lin in self.lins:\n",
    "        #     lin.reset_parameters()\n",
    "        self.bilin.reset_parameters()\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        # for lin in self.lins:\n",
    "        #     x_i, x_j = lin(x_i), lin(x_j)\n",
    "        #     x_i, x_j = self.relu(x_i), self.relu(x_j)\n",
    "        x = torch.sum(self.bilin(x_i) * x_j, dim=-1)\n",
    "        # x1 = torch.sum(self.bilin(x_i) * x_j, dim=-1)\n",
    "        # x2 = torch.sum(self.bilin(x_j) * x_i, dim=-1)\n",
    "        # x = x1+x2\n",
    "        # x = torch.sum(x_i * x_j, dim=-1)\n",
    "        return x\n",
    "\n",
    "class HGTmt(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels, out_channels, num_heads, num_layers,node_types,metadata):\n",
    "        super(HGTmt, self).__init__()\n",
    "        self.hgt=HGT(hidden_channels, out_channels, num_heads, num_layers,node_types,metadata)\n",
    "        self.predict=MLPBilPredictor(hidden_channels[-1], out_channels, 1, 0.3)\n",
    "\n",
    "    def encoder(self,x_dict,edge_index_dict):\n",
    "        xm,xg=self.hgt(x_dict,edge_index_dict)\n",
    "        #print(xm.size(),xg.size())\n",
    "        return xm,xg\n",
    "\n",
    "    def decoder(self,xm,xg,edge):\n",
    "        xm=xm[edge[0]]\n",
    "        xg=xg[edge[1]]\n",
    "        s=self.predict(xm,xg)\n",
    "        return s\n",
    "\n",
    "    def forward(self,x_dict,edge_index_dict,label_edge):\n",
    "        xm,xg=self.encoder(x_dict,edge_index_dict)\n",
    "        s=self.decoder(xm,xg,label_edge)\n",
    "        return s\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self,alpha=0.25,gamma=2.0,reduce='mean'):\n",
    "        super(FocalLoss,self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self,classifications,targets):\n",
    "        # classifcation:[N,K]\n",
    "        # targets: [N,K]的one-hot编码\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "        # classifications = classifications.view(-1)  # 不经过sigmoid的classification；\n",
    "        # targets = targets.view(-1)                  # 应该是 one-hot\n",
    "        # ce_loss: 对应公式中 -log(pt),也就是普通的 交叉熵损失；--> 该函数接收未经sigmoid的函数；\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(classifications, targets, reduction=\"none\")\n",
    "        #focal loss\n",
    "        p = torch.sigmoid(classifications)                # 经过sigmoid\n",
    "        p_t = p * targets + (1 - p) * (1 - targets)       #  计算pt\n",
    "        loss = ce_loss * ((1 - p_t) ** gamma)             # -log(pt) * (1-pt) ** ganmma\n",
    "        if alpha >= 0:\n",
    "            # 对应公式中alpha_t控制损失的权重\n",
    "            alpha_t = alpha * targets + (1 - alpha) * (1 - targets) # 和pt求解过程一样\n",
    "            loss = alpha_t * loss                         # 最终focal loss\n",
    "        if self.reduce=='sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduce=='mean':\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            raise ValueError('reduce type is wrong!')\n",
    "        return loss\n",
    "\n",
    "class BCEFocalLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2, alpha=0.25, reduction='elementwise_mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, _input, target):\n",
    "        pt = torch.sigmoid(_input)\n",
    "        loss = - self.alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - \\\n",
    "               (1-self.alpha)*pt ** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "        #         loss = - self.alpha * pt ** self.gamma * target * torch.log(pt) - \\\n",
    "        #             (1-self.alpha)*(1 - pt)** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "        if self.reduction == 'elementwise_mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "def trainEpoch(model,trainloader,scaler,lossF,device,xm,xg):\n",
    "    model.train()\n",
    "    trainloss=0\n",
    "    auc=0\n",
    "    aupr=0\n",
    "    #acc=0\n",
    "    for step,dat in enumerate(trainloader):\n",
    "        #print(device)\n",
    "        #dat=dat.to(device)\n",
    "        # dat['miRNA'].x=xm\n",
    "        # dat['gene'].x=xg\n",
    "        dat=T.ToUndirected()(dat)\n",
    "        dat=dat.to(device)\n",
    "        #print(dat)\n",
    "        out = model(dat.x_dict,dat.edge_index_dict,dat['regulate'].edge_label_index)\n",
    "        out=out.view(-1)\n",
    "        #print(out.size())\n",
    "        rel=dat['regulate'].edge_label\n",
    "        #print(rel.size())\n",
    "        #loss = F.binary_cross_entropy_with_logits(out,rel)\n",
    "        loss=lossF(out,rel)\n",
    "        #metric=model_evaluation(pre,rel.int())\n",
    "        #print(out)\n",
    "        pre=torch.sigmoid(out)\n",
    "        #print(pre.size())\n",
    "        #assert torch.isnan(loss).sum() == 0, print(loss)\n",
    "        metric0=roc_auc_score(rel.cpu().detach().numpy(), pre.cpu().detach().numpy())\n",
    "        metric1= average_precision_score(rel.cpu().detach().numpy(), pre.cpu().detach().numpy())\n",
    "        #         scaler.scale(loss).backward()\n",
    "        #         scaler.step(optimizer)  # optimizer.step\n",
    "        #         scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss = trainloss+loss.item()\n",
    "        #acc=acc+metric['accuracy']\n",
    "        auc=auc+metric0\n",
    "        aupr=aupr+metric1\n",
    "        # if (step % 100 == 0):\n",
    "        #     train_loss = (trainloss / (step+1))\n",
    "        #     auc_batch = (auc / (step+1))\n",
    "        #     aupr_batch = (aupr / (step+1))\n",
    "        #     #train_loss.append(trainloss)\n",
    "        #     print('Batch:',step,train_loss,auc_batch,aupr_batch)\n",
    "    return (trainloss/(step+1),auc/(step+1),aupr/(step+1))\n",
    "\n",
    "def evaluate(model,valloader,lossF,device,xm,xg):\n",
    "    model.eval()\n",
    "    valloss=0\n",
    "    valauc=0\n",
    "    valaupr=0\n",
    "    with torch.no_grad():\n",
    "        for step,dat in enumerate(valloader):\n",
    "            #print(device)\n",
    "            # dat['miRNA'].x=xm\n",
    "            # dat['gene'].x=xg\n",
    "            dat=T.ToUndirected()(dat)\n",
    "            #print(step)\n",
    "            dat=dat.to(device)\n",
    "            out = model(dat.x_dict,dat.edge_index_dict,dat['regulate'].edge_label_index)\n",
    "            out=out.view(-1)\n",
    "            #print(out.size())\n",
    "            rel=dat['regulate'].edge_label\n",
    "            #print(rel.size())\n",
    "            #loss = F.binary_cross_entropy_with_logits(out,rel)\n",
    "            loss=lossF(out,rel)\n",
    "            #metric=model_evaluation(pre,rel.int())\n",
    "            #print(rel,pre)\n",
    "            pre=torch.sigmoid(out)\n",
    "            #print(pre.size())\n",
    "            auc=roc_auc_score(rel.cpu(), pre.cpu())\n",
    "            aupr= average_precision_score(rel.cpu(), pre.cpu())\n",
    "            valloss = valloss+loss.item()\n",
    "            valauc=valauc+auc\n",
    "            valaupr=valaupr+aupr\n",
    "            # if (step % 100 == 0):\n",
    "            #     val_loss = (valloss / (step+1))\n",
    "            #     #val_loss.append(valloss)\n",
    "            #     val_auc=(valauc/(step+1))\n",
    "            #     val_aupr=(valaupr/(step+1))\n",
    "            #     print('Batch:',step,val_loss,val_auc,val_aupr)\n",
    "    return (valloss/(step+1)),(valauc/(step+1)),(valaupr/(step+1))\n",
    "\n",
    "# class EarlyStopping():\n",
    "#     def __init__(self, tolerance=5, min_delta=0.1):\n",
    "\n",
    "#         self.tolerance = tolerance\n",
    "#         self.min_delta = min_delta\n",
    "#         self.counter = 0\n",
    "#         self.early_stop = False\n",
    "\n",
    "#     def __call__(self, train_auc, validation_auc):\n",
    "#         if (train_auc - validation_auc) > self.min_delta:\n",
    "#             self.counter +=1\n",
    "#             if self.counter >= self.tolerance:\n",
    "#                 self.early_stop = True\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=50, min_delta=0.1):\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_best_auc, val_auc):\n",
    "        if val_auc<val_best_auc:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     torch.cuda.set_device(0)\n",
    "#     set_seed(2022)\n",
    "        \n",
    "#     with open('dataCombine_negall.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#         train_data,val_data,_,m,g = pickle.load(f)\n",
    "\n",
    "#     train_data ['miRNA'].sim = train_data ['miRNA'].mm + train_data ['miRNA'].x\n",
    "#     val_data ['miRNA'].sim = train_data ['miRNA'].mm + train_data ['miRNA'].x\n",
    "#     #test_data ['miRNA'].sim = train_data ['miRNA'].mm + train_data ['miRNA'].x\n",
    "#     train_data ['gene'].sim = train_data ['gene'].gg + train_data ['gene'].x\n",
    "#     val_data ['gene'].sim = train_data ['gene'].gg + train_data ['gene'].x\n",
    "#     #test_data ['gene'].sim = train_data ['gene'].gg + train_data ['gene'].x\n",
    "#     nodetypes=train_data.node_types\n",
    "#     metadata=train_data.metadata()\n",
    "\n",
    "#     train_data['miRNA'].x=train_data['miRNA'].sim\n",
    "#     train_data['gene'].x=train_data['gene'].sim\n",
    "#     val_data['miRNA'].x=val_data['miRNA'].sim\n",
    "#     val_data['gene'].x=val_data['gene'].sim\n",
    "\n",
    "#     train_data['miRNA'].num_nodes = len(m)\n",
    "#     train_data['gene'].num_nodes = len(g)\n",
    "#     val_data['miRNA'].num_nodes = len(m)\n",
    "#     val_data['gene'].num_nodes = len(g)\n",
    "\n",
    "#     del train_data['miRNA'].sim\n",
    "#     del train_data['miRNA'].mm\n",
    "#     del train_data['miRNA'].seq\n",
    "#     del train_data['gene'].sim\n",
    "#     del train_data['gene'].gg\n",
    "#     del train_data['gene'].seq\n",
    "#     del train_data['rev_regulate']\n",
    "\n",
    "#     del val_data['miRNA'].sim\n",
    "#     del val_data['miRNA'].mm\n",
    "#     del val_data['miRNA'].seq\n",
    "#     del val_data['gene'].sim\n",
    "#     del val_data['gene'].gg\n",
    "#     del val_data['gene'].seq\n",
    "#     del val_data['rev_regulate']\n",
    "\n",
    "#     train_loader = LinkNeighborLoader(\n",
    "#         data=train_data,\n",
    "#         num_neighbors={('miRNA','regulate','gene'):[8]*4,('miRNA','cofamily','miRNA'):[8]*4,('gene','coocurrence','gene'):[8]*4},\n",
    "#         #num_neighbors={('miRNA','regulate','gene'):[-1]*1,('miRNA','cofamily','miRNA'):[-1]*1,('gene','coocurrence','gene'):[-1]*1},\n",
    "#         #edge_label_index=('miRNA','regulate','gene'),\n",
    "#         #num_neighbors=[-1]*1,\n",
    "#         #num_neighbors=[8]*4,\n",
    "#         edge_label_index=(('miRNA','regulate','gene'),train_data['regulate'].edge_label_index),\n",
    "#         edge_label=train_data['regulate'].edge_label,\n",
    "#         batch_size=1024*2,\n",
    "#         #weight_attr=None,\n",
    "#         shuffle=True,\n",
    "#         #num_workers=8, \n",
    "#         pin_memory=True,\n",
    "#     )\n",
    "#     val_loader = LinkNeighborLoader(\n",
    "#         data=val_data,\n",
    "#         num_neighbors={('miRNA','regulate','gene'):[8]*4,('miRNA','cofamily','miRNA'):[8]*4,('gene','coocurrence','gene'):[8]*4},\n",
    "#         #num_neighbors={('miRNA','regulate','gene'):[-1]*1,('miRNA','cofamily','miRNA'):[-1]*1,('gene','coocurrence','gene'):[-1]*1},\n",
    "#         #edge_label_index=('miRNA','regulate','gene'),\n",
    "#         #num_neighbors=[-1]*1,\n",
    "#         #num_neighbors=[8]*4,\n",
    "#         edge_label_index=(('miRNA','regulate','gene'),val_data['regulate'].edge_label_index),\n",
    "#         edge_label=val_data['regulate'].edge_label,\n",
    "#         batch_size=1024*2,\n",
    "#         #weight_attr=None,\n",
    "#         shuffle=True,\n",
    "#         #num_workers=8, \n",
    "#         pin_memory=True,\n",
    "#     )\n",
    "\n",
    "#     xm=train_data['miRNA'].x\n",
    "#     xg=train_data['gene'].x\n",
    "#     model = HGTmt(hidden_channels=[1024,256,128], out_channels=128, num_heads=8, num_layers=2,node_types=nodetypes,metadata=metadata)\n",
    "#     #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     #device='cpu'\n",
    "#     device='cuda:0'\n",
    "#     #print(device)\n",
    "#     model.to(device)\n",
    "#     print(model)\n",
    "    \n",
    "#     #torch.cuda.set_device(1)\n",
    "#     #set_seed(2022)\n",
    "#     for m in model.modules():\n",
    "#         if isinstance(m, (torch.nn.Linear)):\n",
    "#             torch.nn.init.kaiming_normal_(m.weight, mode = 'fan_in')\n",
    "#     #weight_decay=5e-4\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-3)\n",
    "#     scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "#     #0.1 0.05 0.01 0.005 0.0001\n",
    "#     #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100, eta_min=0.000001)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max',factor=0.5,verbose=1,min_lr=0.000001,patience=20)\n",
    "#     lossF=FocalLoss(alpha=0.65, gamma=2,reduce='mean')\n",
    "#     #lossF=FocalLoss(alpha=-1, gamma=0.2,reduce='mean')\n",
    "#     #lossF=BCEFocalLoss(gamma=1, alpha=0.75, reduction='elementwise_mean')\n",
    "#     early_stopping = EarlyStopping(tolerance=20, min_delta=0.15)\n",
    "\n",
    "#     # Define the early stopping parameters\n",
    "#     patience = 50\n",
    "#     #best_va = float('inf')\n",
    "#     counter = 0\n",
    "#     #print(device)\n",
    "#     import time\n",
    "#     best_val_auc= best_val_aupr= 0\n",
    "#     best_epoch=-1\n",
    "#     trainloss=[]\n",
    "#     valloss=[]\n",
    "#     valauc=[]\n",
    "#     trainauc=[]\n",
    "#     valaupr=[]\n",
    "#     trainaupr=[]\n",
    "#     lrchange=[]\n",
    "#     for epoch in range(1, 1001):\n",
    "#         since = time.time()\n",
    "#         print('{} optim: {}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "#         lrchange.append(optimizer.param_groups[0]['lr'])\n",
    "#         train_loss,train_auc,train_aupr = trainEpoch(model,train_loader,scaler,lossF,device,xm,xg)\n",
    "#         trainloss.append(train_loss)\n",
    "#         print('train_loss:',train_loss)\n",
    "#         val_loss,val_auc,val_aupr = evaluate(model,val_loader,lossF,device,xm,xg)\n",
    "#         valauc.append(val_auc)\n",
    "#         valloss.append(val_loss)\n",
    "#         trainauc.append(train_auc)\n",
    "#         valaupr.append(val_aupr)\n",
    "#         trainaupr.append(train_aupr)\n",
    "#         #time_elapsed = time.time() - since1\n",
    "#         #print('Val and Testing in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#         #scheduler.step() \n",
    "#         scheduler.step(val_aupr)\n",
    "#         if val_aupr > best_val_aupr:\n",
    "#             best_val_auc = val_auc\n",
    "#             best_val_aupr=val_aupr\n",
    "#             counter = 0\n",
    "#             best_epoch = epoch\n",
    "#             #print(f'best aupr {epoch}')\n",
    "#             torch.save(model, './HGT/HGTbest_linkloader_ReduceLR_2.pt')\n",
    "#             time_elapsed = time.time() - since\n",
    "#             log = 'Epoch: {:03d}, Epoch complete in {:.0f}m {:.0f}s, trainLoss: {:.4f}, Valloss: {:.4f}, Trainauc: {:.4f}, Valauc: {:.4f}, Valbestauc: {:.4f},Trainaupr: {:.4f}, Valaupr: {:.4f}, Valbestaupr: {:.4f}'\n",
    "#             print(log.format(epoch, time_elapsed // 60, time_elapsed % 60,train_loss, val_loss,train_auc,val_auc,best_val_auc,train_aupr,val_aupr,best_val_aupr))\n",
    "#         else:\n",
    "#             counter += 1\n",
    "#             time_elapsed = time.time() - since\n",
    "#             log = 'Epoch: {:03d}, Epoch complete in {:.0f}m {:.0f}s, trainLoss: {:.4f}, Valloss: {:.4f}, Trainauc: {:.4f}, Valauc: {:.4f}, Valbestauc: {:.4f},Trainaupr: {:.4f}, Valaupr: {:.4f}, Valbestaupr: {:.4f}'\n",
    "#             print(log.format(epoch, time_elapsed // 60, time_elapsed % 60,train_loss, val_loss,train_auc,val_auc,best_val_auc,train_aupr,val_aupr,best_val_aupr))\n",
    "#             if counter >= patience:\n",
    "#                 print(f'Early stopping at epoch {epoch}')\n",
    "#                 print(f'best aupr at epoch {best_epoch}')\n",
    "#                 break\n",
    "                \n",
    "#         # scheduler.step() \n",
    "#         # #scheduler.step(val_aupr)\n",
    "#         # time_elapsed = time.time() - since\n",
    "#         # log = 'Epoch: {:03d}, Epoch complete in {:.0f}m {:.0f}s, trainLoss: {:.4f}, Valloss: {:.4f}, Trainauc: {:.4f}, Valauc: {:.4f}, Valbestauc: {:.4f},Trainaupr: {:.4f}, Valaupr: {:.4f}, Valbestaupr: {:.4f}'\n",
    "#         # print(log.format(epoch+1, time_elapsed // 60, time_elapsed % 60,train_loss, val_loss,train_auc,val_auc,best_val_auc,train_aupr,val_aupr,best_val_aupr))\n",
    "\n",
    "#     with open('./HGT/HGTResult_linkloader_ReduceLR_2.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#         pickle.dump([trainloss, trainauc,trainaupr,valloss,valauc,valaupr,lrchange], f)\n",
    "\n",
    "#     torch.save(model, './HGT/HGT_linkloader_ReduceLR_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5478df-6800-4a4f-9ffe-763a3912236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGTmt(\n",
      "  (hgt): HGT(\n",
      "    (lin_dict): ModuleDict(\n",
      "      (miRNA): Linear(2656, 1024, bias=True)\n",
      "      (gene): Linear(18454, 1024, bias=True)\n",
      "    )\n",
      "    (convs): ModuleList(\n",
      "      (0): HGTConv(-1, 256, heads=8)\n",
      "      (1): HGTConv(-1, 128, heads=8)\n",
      "    )\n",
      "    (relu): GELU(approximate='none')\n",
      "  )\n",
      "  (predict): MLPBilPredictor(\n",
      "    (bilin): Linear(in_features=128, out_features=128, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "set_seed(2022)\n",
    "#HGT = torch.load(\"./HGT/HGTbest_linkloader_ReduceLR_2_2.pt\")\n",
    "#HGT = torch.load(\"./results/hnuResults/HGT/HGTbest_linkloader_ReduceLR_2.pt\") \n",
    "HGT = torch.load(\"./HGT/HGTbest_cpu_ReduceLR_usingmiRNAanchor.pt\")\n",
    "print(HGT)\n",
    "torch.save(HGT.state_dict(),'./results/HGTfull_usingmiRNAanchor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6879511-bd70-40f3-9239-47bf8250e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataCombine_negall.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    train_data,val_data,test_data,m,g = pickle.load(f)\n",
    "\n",
    "train_data ['miRNA'].sim = train_data ['miRNA'].mm + train_data ['miRNA'].x\n",
    "val_data ['miRNA'].sim = train_data ['miRNA'].mm + train_data ['miRNA'].x\n",
    "test_data ['miRNA'].sim = train_data ['miRNA'].mm + train_data ['miRNA'].x\n",
    "train_data ['gene'].sim = train_data ['gene'].gg + train_data ['gene'].x\n",
    "val_data ['gene'].sim = train_data ['gene'].gg + train_data ['gene'].x\n",
    "test_data ['gene'].sim = train_data ['gene'].gg + train_data ['gene'].x\n",
    "nodetypes=train_data.node_types\n",
    "metadata=train_data.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32853d6e-6075-48d7-9666-61fd6fca2b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 hgt.lin_dict.miRNA.weight : torch.Size([1024, 2656])\n",
      "1 hgt.lin_dict.miRNA.bias : torch.Size([1024])\n",
      "2 hgt.lin_dict.gene.weight : torch.Size([1024, 18454])\n",
      "3 hgt.lin_dict.gene.bias : torch.Size([1024])\n",
      "4 hgt.convs.0.kqv_lin.lins.miRNA.weight : torch.Size([768, 1024])\n",
      "5 hgt.convs.0.kqv_lin.lins.miRNA.bias : torch.Size([768])\n",
      "6 hgt.convs.0.kqv_lin.lins.gene.weight : torch.Size([768, 1024])\n",
      "7 hgt.convs.0.kqv_lin.lins.gene.bias : torch.Size([768])\n",
      "8 hgt.convs.0.out_lin.lins.miRNA.weight : torch.Size([256, 256])\n",
      "9 hgt.convs.0.out_lin.lins.miRNA.bias : torch.Size([256])\n",
      "10 hgt.convs.0.out_lin.lins.gene.weight : torch.Size([256, 256])\n",
      "11 hgt.convs.0.out_lin.lins.gene.bias : torch.Size([256])\n",
      "12 hgt.convs.0.k_rel.weight : torch.Size([32, 32, 32])\n",
      "13 hgt.convs.0.v_rel.weight : torch.Size([32, 32, 32])\n",
      "14 hgt.convs.0.skip.gene : torch.Size([1])\n",
      "15 hgt.convs.0.skip.miRNA : torch.Size([1])\n",
      "16 hgt.convs.0.p_rel.miRNA__regulate__gene : torch.Size([1, 8])\n",
      "17 hgt.convs.0.p_rel.gene__coocurrence__gene : torch.Size([1, 8])\n",
      "18 hgt.convs.0.p_rel.miRNA__cofamily__miRNA : torch.Size([1, 8])\n",
      "19 hgt.convs.0.p_rel.gene__rev_regulate__miRNA : torch.Size([1, 8])\n",
      "20 hgt.convs.1.kqv_lin.lins.miRNA.weight : torch.Size([384, 256])\n",
      "21 hgt.convs.1.kqv_lin.lins.miRNA.bias : torch.Size([384])\n",
      "22 hgt.convs.1.kqv_lin.lins.gene.weight : torch.Size([384, 256])\n",
      "23 hgt.convs.1.kqv_lin.lins.gene.bias : torch.Size([384])\n",
      "24 hgt.convs.1.out_lin.lins.miRNA.weight : torch.Size([128, 128])\n",
      "25 hgt.convs.1.out_lin.lins.miRNA.bias : torch.Size([128])\n",
      "26 hgt.convs.1.out_lin.lins.gene.weight : torch.Size([128, 128])\n",
      "27 hgt.convs.1.out_lin.lins.gene.bias : torch.Size([128])\n",
      "28 hgt.convs.1.k_rel.weight : torch.Size([32, 16, 16])\n",
      "29 hgt.convs.1.v_rel.weight : torch.Size([32, 16, 16])\n",
      "30 hgt.convs.1.skip.gene : torch.Size([1])\n",
      "31 hgt.convs.1.skip.miRNA : torch.Size([1])\n",
      "32 hgt.convs.1.p_rel.miRNA__regulate__gene : torch.Size([1, 8])\n",
      "33 hgt.convs.1.p_rel.gene__coocurrence__gene : torch.Size([1, 8])\n",
      "34 hgt.convs.1.p_rel.miRNA__cofamily__miRNA : torch.Size([1, 8])\n",
      "35 hgt.convs.1.p_rel.gene__rev_regulate__miRNA : torch.Size([1, 8])\n",
      "36 predict.bilin.weight : torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for name,parameters in HGT.named_parameters():\n",
    "    print(i,name,':',parameters.size())\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfd9893-383e-4c6d-a8e7-465b6dc10d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HGT(\n",
       "  (lin_dict): ModuleDict(\n",
       "    (miRNA): Linear(2656, 1024, bias=True)\n",
       "    (gene): Linear(18454, 1024, bias=True)\n",
       "  )\n",
       "  (convs): ModuleList(\n",
       "    (0): HGTConv(-1, 256, heads=8)\n",
       "    (1): HGTConv(-1, 128, heads=8)\n",
       "  )\n",
       "  (relu): GELU(approximate='none')\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HGT.hgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8dc072-6c1a-4f32-baa6-8274dd967360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2656, 128]) torch.Size([18454, 128])\n"
     ]
    }
   ],
   "source": [
    "HGT.hgt.eval()\n",
    "HGT = HGT.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    x_dict = HGT.hgt(train_data.sim_dict,train_data.edge_index_dict)\n",
    "\n",
    "xm=x_dict[0]\n",
    "xg=x_dict[1]\n",
    "print(xm.size(),xg.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda20722-1a3a-481e-ad88-5c1343cc14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HGT.hgt.eval()\n",
    "# HGT = HGT.to(\"cuda:0\")\n",
    "# #del train_data['miRNA'].sim\n",
    "# del train_data['miRNA'].mm\n",
    "# del train_data['miRNA'].seq\n",
    "# #del train_data['gene'].sim\n",
    "# del train_data['gene'].gg\n",
    "# del train_data['gene'].seq\n",
    "# del train_data['rev_regulate']\n",
    "# train_data=train_data.to(\"cuda:0\")\n",
    "# with torch.no_grad():\n",
    "#     x_dict = HGT.hgt(train_data.sim_dict,train_data.edge_index_dict)\n",
    "\n",
    "# xm=x_dict[0]\n",
    "# xg=x_dict[1]\n",
    "# print(xm.size(),xg.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4334dec6-f4da-44ae-bafd-d203e95fbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HGTlinkloader_mgEmbedding.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([xm,xg], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815dcb1-5278-43a1-90ce-b360978d534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/HGTlinkloader_mgEmbedding_usingmiRNAanchor.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([xm,xg], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32f6cb73-60bb-4318-8be5-2fbc82f2331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/HGTfull_mgEmbedding_usingmiRNAanchor.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([xm,xg], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b234de-9d89-4e9a-8413-df9b3857a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "#import rpy2.robjects as robjects\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from torchvision import transforms as tfs\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "#from torch_geometric.loader import HGTLoader\n",
    "from torch.cuda.amp import autocast\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from torchsummary import summary\n",
    "import random\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, precision_recall_curve, average_precision_score\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import math\n",
    "#from einops import rearrange, reduce\n",
    "#from einops.layers.torch import Rearrange\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class eca_layer(nn.Module):\n",
    "    \"\"\"Constructs a ECA module.\n",
    "    Args:\n",
    "        channel: Number of channels of the input feature map\n",
    "        k_size: Adaptive selection of kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, k_size=3):\n",
    "        super(eca_layer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # feature descriptor on the global spatial information\n",
    "        #print(x.size())\n",
    "        y = self.avg_pool(x)\n",
    "        #print(y.size())\n",
    "        # Two different branches of ECA module\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        #print(y.size())\n",
    "        # Multi-scale information fusion\n",
    "        y = self.sigmoid(y)\n",
    "        #print(y.size())\n",
    "        out=x * y.expand_as(x)\n",
    "        #print(out.size())\n",
    "        return out\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def conv_kx1(in_channels, out_channels, kernel_size, stride=1):\n",
    "    layers = []\n",
    "    padding = kernel_size - 1\n",
    "    padding_left = padding // 2\n",
    "    padding_right = padding - padding_left\n",
    "    layers.append(nn.ConstantPad1d((padding_left, padding_right), 0))\n",
    "    layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, stride))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv_kx2(in_channels, out_channels, kernel_size, stride=1):\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Conv2_Layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(Conv2_Layer, self).__init__()\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.eca1=eca_layer(in_channels,3)\n",
    "        self.conv1 = conv_kx2(in_channels, out_channels[0], kernel_size)\n",
    "        self.norm1=nn.BatchNorm2d(out_channels[0])\n",
    "        self.eca2=eca_layer(out_channels[0],3)\n",
    "        self.conv2 = conv_kx2(out_channels[0], out_channels[1], kernel_size)\n",
    "        self.norm2=nn.BatchNorm2d(out_channels[1])\n",
    "        self.eca3=eca_layer(out_channels[1],3)\n",
    "        self.relu=nn.GELU()\n",
    "    def forward(self, x):\n",
    "        #out = self.eca1(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.eca2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.eca3(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def ConvBlock(dim, dim_out = None, kernel_size = 1):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm1d(dim),\n",
    "        nn.GELU(),\n",
    "        conv_kx1(dim, default(dim_out, dim), kernel_size))\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout = 0.3, max_len=35526):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * (-(math.log(10000.0) / d_model)))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)],requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class gCNN(nn.Module):\n",
    "    def __init__(self,max_len=2500,dim=128,nhead=4,num_layers=2,pool_size=[(3,1),3,11],out_channels=[7,1],stem_kernel_size=[(7,1),7],dropout_rate=0.3):\n",
    "        super(gCNN, self).__init__()\n",
    "        self.stem1 = nn.Sequential(\n",
    "            Conv2_Layer(15,out_channels,stem_kernel_size[0]),\n",
    "            nn.MaxPool2d(pool_size[0])\n",
    "        )\n",
    "        out_length = np.floor((((max_len-(stem_kernel_size[0][0]*2)+2) - pool_size[0][0]) / pool_size[0][0]) + 1)\n",
    "        print(out_length)\n",
    "        self.stem2 = nn.Sequential(\n",
    "            conv_kx1(4, dim, stem_kernel_size[1]),\n",
    "            Residual(ConvBlock(dim,dim,stem_kernel_size[1])),\n",
    "            nn.MaxPool1d(pool_size[1])\n",
    "        )\n",
    "        out_length = np.floor(((out_length - pool_size[1]) / pool_size[1]) + 1)\n",
    "        print(out_length)\n",
    "        self.stem3 = nn.Sequential(\n",
    "            #conv_kx1(64, dim, stem_kernel_size[1]),\n",
    "            Residual(ConvBlock(dim,dim,stem_kernel_size[1])),\n",
    "            nn.MaxPool1d(kernel_size=pool_size[2],stride=10)\n",
    "        )\n",
    "        #out_length = round(((out_length - pool_size[2]) / 1) + 1)\n",
    "        out_length = np.floor((out_length - pool_size[2]) / 10) + 1\n",
    "        print(out_length)\n",
    "        self.position = PositionalEncoding(d_model=dim,max_len=int(out_length))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=nhead,batch_first=True,dim_feedforward=256,activation='gelu',dropout=dropout_rate)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # self.linear = nn.Linear(int(dim * out_length), 1024)\n",
    "        # self.relu = nn.GELU()\n",
    "        # self.dropout = nn.Dropout(p=dropout_rate if dropout_rate is not None else 0)\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = self.stem1(x)\n",
    "        #print(x.size())\n",
    "        x=x.view(x.size()[0],x.size()[2],x.size()[3])\n",
    "        #print(x.size())\n",
    "        x=x.permute(0,2,1)\n",
    "        #print(x.size())\n",
    "        x=self.stem2(x)\n",
    "        #print(x.size())\n",
    "        x=self.stem3(x)\n",
    "        #print(x.size())\n",
    "        x=x.permute(0, 2, 1)\n",
    "        #print(x.size())\n",
    "        x = self.position(x)\n",
    "        #print(x.size())\n",
    "        x = self.transformer_encoder(x)\n",
    "        #print(x.size())\n",
    "        x = x.reshape(len(x), -1)\n",
    "        #print(x.size())\n",
    "        # x = self.linear(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        #print(x.size())\n",
    "        return x\n",
    "\n",
    "class mCNN(nn.Module):\n",
    "    def __init__(self,max_len=28,dim=128,nhead=8,num_layers=2,stem_kernel_size=7,dropout_rate=0.3):\n",
    "        super(mCNN, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            #nn.Conv1d(4, dim, 7),\n",
    "            conv_kx1(4, dim, stem_kernel_size),\n",
    "            Residual(ConvBlock(dim,dim,stem_kernel_size))\n",
    "            #AttentionPool(dim, pool_size = 2)\n",
    "        )\n",
    "        self.position = PositionalEncoding(d_model=dim,max_len=max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=nhead,batch_first=True,dim_feedforward=256,activation='gelu',dropout=dropout_rate)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # self.linear = nn.Linear(int(dim * max_len), 1024)\n",
    "        # self.relu = nn.GELU()\n",
    "        # self.dropout = nn.Dropout(p=dropout_rate if dropout_rate is not None else 0)\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x=x.permute(0, 2, 1)\n",
    "        #print(x.size())\n",
    "        x = self.stem(x)\n",
    "        #print(x.size())\n",
    "        #x = self.conv_tower(x)\n",
    "        #print(x.size())\n",
    "        x=x.permute(0, 2, 1)\n",
    "        #print(x.size())\n",
    "        x = self.position(x)\n",
    "        #print(x.size())\n",
    "        x = self.transformer_encoder(x)\n",
    "        #print(x.size())\n",
    "        x = x.reshape(len(x), -1)\n",
    "        #print(x.size())\n",
    "        # x = self.linear(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        #print(x.size())\n",
    "        return x\n",
    "\n",
    "class MLPBilPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, dropout_rate=0.3):\n",
    "        super(MLPBilPredictor, self).__init__()\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels[0]))\n",
    "        for i in range(num_layers - 1):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels[i], hidden_channels[i+1]))\n",
    "        self.bilin = torch.nn.Linear(hidden_channels[-1], hidden_channels[-1], bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate if dropout_rate is not None else 0)\n",
    "        # self.bilin = torch.nn.Linear(in_channels,in_channels, bias=False)\n",
    "        self.relu = nn.GELU()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        self.bilin.reset_parameters()\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        for lin in self.lins:\n",
    "            x_i, x_j = lin(x_i), lin(x_j)\n",
    "            x_i, x_j = self.dropout(self.relu(x_i)), self.dropout(self.relu(x_j))\n",
    "        x = torch.sum(self.bilin(x_i) * x_j, dim=-1)\n",
    "        # x1 = torch.sum(self.bilin(x_i) * x_j, dim=-1)\n",
    "        # x2 = torch.sum(self.bilin(x_j) * x_i, dim=-1)\n",
    "        # x = x1+x2\n",
    "        # x = torch.sum(x_i * x_j, dim=-1)\n",
    "        return x,x_i,x_j\n",
    "\n",
    "# class MLPBilPredictor(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, num_layers, dropout_rate=0.3):\n",
    "#         super(MLPBilPredictor, self).__init__()\n",
    "#         self.lins = torch.nn.ModuleList()\n",
    "#         self.lins.append(torch.nn.Linear(in_channels, hidden_channels[0]))\n",
    "#         for i in range(num_layers - 1):\n",
    "#             self.lins.append(torch.nn.Linear(hidden_channels[i], hidden_channels[i+1]))\n",
    "#         # self.bilin = torch.nn.Linear(hidden_channels, hidden_channels, bias=False)\n",
    "#         # self.bilin = torch.nn.Linear(in_channels,in_channels, bias=False)\n",
    "#         self.pre = torch.nn.Linear(hidden_channels[-1],1)\n",
    "#         self.relu = nn.GELU()\n",
    "#         self.dropout = nn.Dropout(p=dropout_rate if dropout_rate is not None else 0)\n",
    "#\n",
    "#     def reset_parameters(self):\n",
    "#         for lin in self.lins:\n",
    "#             lin.reset_parameters()\n",
    "#         self.lin.reset_parameters()\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         for lin in self.lins:\n",
    "#             x= lin(x)\n",
    "#             x= self.dropout(self.relu(x))\n",
    "#         x = self.pre(x)\n",
    "#         # x1 = torch.sum(self.bilin(x_i) * x_j, dim=-1)\n",
    "#         # x2 = torch.sum(self.bilin(x_j) * x_i, dim=-1)\n",
    "#         # x = x1+x2\n",
    "#         # x = torch.sum(x_i * x_j, dim=-1)\n",
    "#         return x\n",
    "\n",
    "class CNNmt(torch.nn.Module):\n",
    "    def __init__(self,max_len=[25,2500],dim=[128,128],nhead=[4,4],num_layers=[2,2],pool_size=[(3,1),3,11],out_channels=[7,1],stem_kernel_size=[(7,1),7],dropout_rate=0.3):\n",
    "        #max_len=25,dim=128,nhead=8,num_layers=2\n",
    "        super(CNNmt, self).__init__()\n",
    "        self.mcnn = mCNN(max_len[0],dim[0],nhead[0],num_layers[0],stem_kernel_size[1],dropout_rate)\n",
    "        self.gcnn = gCNN(max_len[1],dim[1],nhead[1],num_layers[1],pool_size,out_channels,stem_kernel_size,dropout_rate)\n",
    "        # self.linear = nn.Linear(3200, 1024)\n",
    "        # self.relu = nn.GELU()\n",
    "        # self.dropout = nn.Dropout(p=dropout_rate if dropout_rate is not None else 0)\n",
    "        self.predict=MLPBilPredictor(3584, [1024], 1)\n",
    "        # self.lin1 = Linear(1024*2, 1024)\n",
    "        # self.lin2 = Linear(1024,128)\n",
    "        # self.lin = Linear(128,1)\n",
    "        # #self.relu = nn.LeakyReLU()\n",
    "        # self.relu = nn.GELU()\n",
    "        # #self.sig=nn.Sigmoid()\n",
    "        # self.dropout = nn.Dropout(p=dropout_rate if dropout_rate is not None else 0)\n",
    "    def encoder(self, xm, xg):\n",
    "        xm=self.mcnn(xm)\n",
    "        xg=self.gcnn(xg)\n",
    "        # xm = self.linear(xm)\n",
    "        # xm = self.relu(xm)\n",
    "        # xm = self.dropout(xm)\n",
    "        # xg = self.linear(xg)\n",
    "        # xg = self.relu(xg)\n",
    "        # xg = self.dropout(xg)\n",
    "        return xm,xg\n",
    "    def decoder(self, xm, xg):\n",
    "        #x=torch.cat([xm, xg], dim=1)\n",
    "        s,xm,xg=self.predict(xm, xg)\n",
    "        # x=self.lin1(x)\n",
    "        # x = self.relu(x)dgg\n",
    "        # x = self.dropout(x)\n",
    "        # x=self.lin2(x)\n",
    "        # x = self.relu(x)\n",
    "        # s=self.lin(x)\n",
    "        return s,xm,xg\n",
    "    def forward(self, xm, xg):\n",
    "        xm,xg=self.encoder(xm,xg)\n",
    "        s,xm,xg=self.decoder(xm,xg)\n",
    "        return s,xm,xg\n",
    "\n",
    "def trainEpoch(model,trainloader,scaler,lossF,device):\n",
    "    model.train()\n",
    "    trainloss=0\n",
    "    auc=0\n",
    "    aupr=0\n",
    "    #acc=0\n",
    "    for step,dat in enumerate(trainloader):\n",
    "        xm,xg,mid,gid,rel=dat\n",
    "        xm,xg,mid,gid,rel=xm.to(device),xg.to(device),mid.to(device),gid.to(device),rel.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # 训练模型\n",
    "        #with autocast():\n",
    "        #out = model(dat.x_dict,dat.seq_dict, dat.edge_index_dict)\n",
    "        out,_,_ = model(xm, xg)\n",
    "        out=out.view(-1)\n",
    "        #pre=out[mid,gid]\n",
    "        #print(rel)\n",
    "        #loss = F.binary_cross_entropy_with_logits(out,rel)\n",
    "        loss=lossF(out,rel)\n",
    "        #metric=model_evaluation(pre,rel.int())\n",
    "        #print(out)\n",
    "        pre=torch.sigmoid(out)\n",
    "        #print(pre)\n",
    "        #assert torch.isnan(loss).sum() == 0, print(loss)\n",
    "        metric0=roc_auc_score(rel.cpu().detach().numpy(), pre.cpu().detach().numpy())\n",
    "        metric1= average_precision_score(rel.cpu().detach().numpy(), pre.cpu().detach().numpy())\n",
    "        #         scaler.scale(loss).backward()\n",
    "        #         scaler.step(optimizer)  # optimizer.step\n",
    "        #         scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss = trainloss+loss.item()\n",
    "        #acc=acc+metric['accuracy']\n",
    "        auc=auc+metric0\n",
    "        aupr=aupr+metric1\n",
    "        # if (step % 100 == 0):\n",
    "        #     train_loss = (trainloss / (step+1))\n",
    "        #     auc_batch = (auc / (step+1))\n",
    "        #     aupr_batch = (aupr / (step+1))\n",
    "        #     #train_loss.append(trainloss)\n",
    "        #     print('Batch:',step,train_loss,auc_batch,aupr_batch)\n",
    "    return (trainloss/(step+1),auc/(step+1),aupr/(step+1))\n",
    "\n",
    "def evaluate(model,valloader,lossF,device):\n",
    "    model.eval()\n",
    "    valloss=0\n",
    "    valauc=0\n",
    "    valaupr=0\n",
    "    with torch.no_grad():\n",
    "        for step,dat in enumerate(valloader):\n",
    "            xm,xg,mid,gid,rel=dat\n",
    "            xm,xg,mid,gid,rel=xm.to(device),xg.to(device),mid.to(device),gid.to(device),rel.to(device)\n",
    "            #pre=out[mid,gid]\n",
    "            out,_,_ = model(xm, xg)\n",
    "            out=out.view(-1)\n",
    "            #loss = F.binary_cross_entropy_with_logits(out,rel)\n",
    "            loss=lossF(out,rel)\n",
    "            #metric=model_evaluation(pre,rel.int())\n",
    "            #print(rel,pre)\n",
    "            pre=torch.sigmoid(out)\n",
    "            auc=roc_auc_score(rel.cpu(), pre.cpu())\n",
    "            aupr= average_precision_score(rel.cpu(), pre.cpu())\n",
    "            valloss = valloss+loss.item()\n",
    "            valauc=valauc+auc\n",
    "            valaupr=valaupr+aupr\n",
    "            # if (step % 100 == 0):\n",
    "            #     val_loss = (valloss / (step+1))\n",
    "            #     #val_loss.append(valloss)\n",
    "            #     val_auc=(valauc/(step+1))\n",
    "            #     val_aupr=(valaupr/(step+1))\n",
    "            #     print('Batch:',step,val_loss,val_auc,val_aupr)\n",
    "    return (valloss/(step+1)),(valauc/(step+1)),(valaupr/(step+1))\n",
    "\n",
    "class seqData(Dataset):\n",
    "    def __init__(self,xm,xg,label,edgeidx):\n",
    "        self.source= xm\n",
    "        self.target=xg\n",
    "        self.edge=edgeidx\n",
    "        self.label = label\n",
    "        self.length = len(self.label)\n",
    "    def __getitem__(self, index):\n",
    "        s=self.edge[0][index]\n",
    "        t=self.edge[1][index]\n",
    "        return self.source[s], self.target[t],s,t,self.label[index]\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self,alpha=0.25,gamma=2.0,reduce='mean'):\n",
    "        super(FocalLoss,self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self,classifications,targets):\n",
    "        # classifcation:[N,K]\n",
    "        # targets: [N,K]的one-hot编码\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "        # classifications = classifications.view(-1)  # 不经过sigmoid的classification；\n",
    "        # targets = targets.view(-1)                  # 应该是 one-hot\n",
    "        # ce_loss: 对应公式中 -log(pt),也就是普通的 交叉熵损失；--> 该函数接收未经sigmoid的函数；\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(classifications, targets, reduction=\"none\")\n",
    "        #focal loss\n",
    "        p = torch.sigmoid(classifications)                # 经过sigmoid\n",
    "        p_t = p * targets + (1 - p) * (1 - targets)       #  计算pt\n",
    "        loss = ce_loss * ((1 - p_t) ** gamma)             # -log(pt) * (1-pt) ** ganmma\n",
    "        if alpha >= 0:\n",
    "            # 对应公式中alpha_t控制损失的权重\n",
    "            alpha_t = alpha * targets + (1 - alpha) * (1 - targets) # 和pt求解过程一样\n",
    "            loss = alpha_t * loss                         # 最终focal loss\n",
    "        if self.reduce=='sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduce=='mean':\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            raise ValueError('reduce type is wrong!')\n",
    "        return loss\n",
    "\n",
    "class BCEFocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25, reduction='elementwise_mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, _input, target):\n",
    "        pt = torch.sigmoid(_input)\n",
    "        loss = - self.alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - \\\n",
    "               (1-self.alpha)*pt ** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "        #         loss = - self.alpha * pt ** self.gamma * target * torch.log(pt) - \\\n",
    "        #             (1-self.alpha)*(1 - pt)** self.gamma * (1 - target) * torch.log(1 - pt)\n",
    "        if self.reduction == 'elementwise_mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "# class EarlyStopping():\n",
    "#     def __init__(self, tolerance=5, min_delta=0.1):\n",
    "#         self.tolerance = tolerance\n",
    "#         self.min_delta = min_delta\n",
    "#         self.counter = 0\n",
    "#         self.early_stop = False\n",
    "#     def __call__(self, train_auc, validation_auc):\n",
    "#         if (train_auc - validation_auc) > self.min_delta:\n",
    "#             self.counter +=1\n",
    "#             if self.counter >= self.tolerance:\n",
    "#                 self.early_stop = True\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=50, min_delta=0.1):\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_best_auc, val_auc):\n",
    "        if val_auc<val_best_auc:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:\n",
    "                self.early_stop = True\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     torch.cuda.set_device(2)\n",
    "#     set_seed(2022)\n",
    "#     with open('dataSplit_negall.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#         Xm,Xg,nodeM,nodeG,trainidx,validx,testidx,trainId,trainLabel,testId,testLabel,valId,valLabel,ppi,m,g = pickle.load(f)\n",
    "\n",
    "#     train_data=seqData(Xm,Xg,trainLabel,trainId)\n",
    "#     #test_data=seqData(Xm,Xg,testLabel,testId)\n",
    "#     val_data=seqData(Xm,Xg,valLabel,valId)\n",
    "\n",
    "#     train_loader = DataLoader(dataset=train_data, batch_size=1024*2, shuffle=True,num_workers=8,pin_memory=True)\n",
    "#     val_loader = DataLoader(dataset=val_data, batch_size=1024*2, shuffle=True,num_workers=8,pin_memory=True)\n",
    "#     #test_loader = DataLoader(dataset=test_data, batch_size=1024*2, shuffle=True,num_workers=8, pin_memory=True)\n",
    "\n",
    "#     model = CNNmt(max_len=[28,2500],dim=[128,128],nhead=[8,8],num_layers=[1,1],pool_size=[(3,1),3,1],out_channels=[7,1],stem_kernel_size=[(7,1),3],dropout_rate=0.3)\n",
    "#     print(model)\n",
    "\n",
    "#     #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     device='cuda:2'\n",
    "#     model.to(device)\n",
    "\n",
    "#     for m in model.modules():\n",
    "#         if isinstance(m, (torch.nn.Linear, torch.nn.Conv1d, torch.nn.Conv2d)):\n",
    "#             torch.nn.init.kaiming_normal_(m.weight, mode = 'fan_in')\n",
    "\n",
    "#     for m in model.modules():\n",
    "#         if isinstance(m, (torch.nn.Linear)):\n",
    "#             torch.nn.init.kaiming_normal_(m.weight, mode = 'fan_in')\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=5e-3)#\n",
    "#     #optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "#     scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "#     #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100, eta_min=0.000001)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max',factor=0.5,verbose=1,min_lr=0.000001,patience=20)\n",
    "#     lossF=FocalLoss(alpha=0.65, gamma=2,reduce='mean')\n",
    "#     #lossF=FocalLoss(alpha=-1, gamma=0.2,reduce='mean')\n",
    "#     #lossF=BCEFocalLoss(gamma=1, alpha=0.75, reduction='elementwise_mean')\n",
    "#     early_stopping = EarlyStopping(tolerance=20, min_delta=0.15)\n",
    "\n",
    "#     # Define the early stopping parameters\n",
    "#     patience = 50\n",
    "#     #best_va = float('inf')\n",
    "#     counter = 0\n",
    "    \n",
    "#     import time\n",
    "#     best_val_auc= best_val_aupr= 0\n",
    "#     best_epoch=-1\n",
    "#     trainloss=[]\n",
    "#     valloss=[]\n",
    "#     valauc=[]\n",
    "#     trainauc=[]\n",
    "#     valaupr=[]\n",
    "#     trainaupr=[]\n",
    "#     lrchange=[]\n",
    "#     for epoch in range(1, 1001):\n",
    "#         since = time.time()\n",
    "#         print('{} optim: {}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "#         lrchange.append(optimizer.param_groups[0]['lr'])\n",
    "#         train_loss,train_auc,train_aupr = trainEpoch(model,train_loader,scaler,lossF,device)\n",
    "#         trainloss.append(train_loss)\n",
    "#         print('train_loss:',train_loss)\n",
    "#         val_loss,val_auc,val_aupr = evaluate(model,val_loader,lossF,device)\n",
    "#         valauc.append(val_auc)\n",
    "#         valloss.append(val_loss)\n",
    "#         trainauc.append(train_auc)\n",
    "#         valaupr.append(val_aupr)\n",
    "#         trainaupr.append(train_aupr)\n",
    "#         #scheduler.step() \n",
    "#         scheduler.step(val_aupr)\n",
    "#         if val_aupr > best_val_aupr:\n",
    "#             best_val_auc = val_auc\n",
    "#             best_val_aupr=val_aupr\n",
    "#             counter = 0\n",
    "#             best_epoch = epoch\n",
    "#             #print(f'best aupr {epoch}')\n",
    "#             torch.save(model, './TransCNN/seqCNNbest_2.pt')\n",
    "#             time_elapsed = time.time() - since\n",
    "#             log = 'Epoch: {:03d}, Epoch complete in {:.0f}m {:.0f}s, trainLoss: {:.4f}, Valloss: {:.4f}, Trainauc: {:.4f}, Valauc: {:.4f}, Valbestauc: {:.4f},Trainaupr: {:.4f}, Valaupr: {:.4f}, Valbestaupr: {:.4f}'\n",
    "#             print(log.format(epoch, time_elapsed // 60, time_elapsed % 60,train_loss, val_loss,train_auc,val_auc,best_val_auc,train_aupr,val_aupr,best_val_aupr))\n",
    "#         else:\n",
    "#             counter += 1\n",
    "#             time_elapsed = time.time() - since\n",
    "#             log = 'Epoch: {:03d}, Epoch complete in {:.0f}m {:.0f}s, trainLoss: {:.4f}, Valloss: {:.4f}, Trainauc: {:.4f}, Valauc: {:.4f}, Valbestauc: {:.4f},Trainaupr: {:.4f}, Valaupr: {:.4f}, Valbestaupr: {:.4f}'\n",
    "#             print(log.format(epoch, time_elapsed // 60, time_elapsed % 60,train_loss, val_loss,train_auc,val_auc,best_val_auc,train_aupr,val_aupr,best_val_aupr))\n",
    "#             if counter >= patience:\n",
    "#                 print(f'Early stopping at epoch {epoch}')\n",
    "#                 print(f'best aupr at epoch {best_epoch}')\n",
    "#                 break\n",
    "        \n",
    "#         # if val_aupr > best_val_aupr:\n",
    "#         #     best_val_auc = val_auc\n",
    "#         #     best_val_aupr=val_aupr\n",
    "#         #     torch.save(model, '/mnt/sda/liupei/miRGraph/TransCNN/seqCNNbest.pt')\n",
    "#         # #scheduler.step()\n",
    "#         # time_elapsed = time.time() - since\n",
    "#         # log = 'Epoch: {:03d}, Epoch complete in {:.0f}m {:.0f}s, trainLoss: {:.4f}, Valloss: {:.4f}, Trainauc: {:.4f}, Valauc: {:.4f}, Valbestauc: {:.4f},Trainaupr: {:.4f}, Valaupr: {:.4f}, Valbestaupr: {:.4f}'\n",
    "#         # print(log.format(epoch, time_elapsed // 60, time_elapsed % 60,train_loss, val_loss,train_auc,val_auc,best_val_auc,train_aupr,val_aupr,best_val_aupr))\n",
    "\n",
    "\n",
    "#     with open('./TransCNN/seqCNNResult_2.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#         pickle.dump([trainloss, trainauc,trainaupr,valloss,valauc,valaupr,lrchange], f)\n",
    "\n",
    "#     torch.save(model, './TransCNN/seqCNN_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ed06d81-b72f-497e-9831-b91a58aa2cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNmt(\n",
      "  (mcnn): mCNN(\n",
      "    (stem): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ConstantPad1d(padding=(1, 1), value=0)\n",
      "        (1): Conv1d(4, 128, kernel_size=(3,), stride=(1,))\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): Sequential(\n",
      "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Sequential(\n",
      "            (0): ConstantPad1d(padding=(1, 1), value=0)\n",
      "            (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (position): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.3, inplace=False)\n",
      "          (dropout2): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gcnn): gCNN(\n",
      "    (stem1): Sequential(\n",
      "      (0): Conv2_Layer(\n",
      "        (conv1): Sequential(\n",
      "          (0): Conv2d(15, 7, kernel_size=(7, 1), stride=(1, 1))\n",
      "        )\n",
      "        (norm1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (eca2): eca_layer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (conv): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (conv2): Sequential(\n",
      "          (0): Conv2d(7, 1, kernel_size=(7, 1), stride=(1, 1))\n",
      "        )\n",
      "        (norm2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (eca3): eca_layer(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (conv): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (relu): GELU(approximate='none')\n",
      "      )\n",
      "      (1): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (stem2): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ConstantPad1d(padding=(1, 1), value=0)\n",
      "        (1): Conv1d(4, 128, kernel_size=(3,), stride=(1,))\n",
      "      )\n",
      "      (1): Residual(\n",
      "        (fn): Sequential(\n",
      "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Sequential(\n",
      "            (0): ConstantPad1d(padding=(1, 1), value=0)\n",
      "            (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (stem3): Sequential(\n",
      "      (0): Residual(\n",
      "        (fn): Sequential(\n",
      "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Sequential(\n",
      "            (0): ConstantPad1d(padding=(1, 1), value=0)\n",
      "            (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): MaxPool1d(kernel_size=1, stride=10, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (position): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.3, inplace=False)\n",
      "          (dropout2): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predict): MLPBilPredictor(\n",
      "    (lins): ModuleList(\n",
      "      (0): Linear(in_features=3584, out_features=1024, bias=True)\n",
      "    )\n",
      "    (bilin): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (relu): GELU(approximate='none')\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#CNN = torch.load(\"./results/hnuResults/TransCNN/seqCNNbest_2.pt\") ##hnu/24g\n",
    "CNN = torch.load(\"./TransCNN/seqCNNbest_2_usingmiRNAanchor.pt\") ##40g\n",
    "#CNN = CNN.to(device)\n",
    "#HGT = HGT.to(\"cpu\")\n",
    "print(CNN)\n",
    "torch.save(CNN.state_dict(),'./results/TransCNN_usingmiRNAanchor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "034c6c04-b1f9-44ff-9859-7ea91a00fd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mCNN(\n",
       "  (stem): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConstantPad1d(padding=(1, 1), value=0)\n",
       "      (1): Conv1d(4, 128, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (1): Residual(\n",
       "      (fn): Sequential(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Sequential(\n",
       "          (0): ConstantPad1d(padding=(1, 1), value=0)\n",
       "          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN.mcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1221e032-7e74-4994-ac59-07de6a13b40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2656, 3584])\n"
     ]
    }
   ],
   "source": [
    "CNN.mcnn.cpu()\n",
    "CNN.mcnn.eval()\n",
    "with torch.no_grad():\n",
    "    xm = CNN.mcnn(train_data['miRNA'].seq)\n",
    "    #xm = CNN.mcnn(Xm)\n",
    "\n",
    "print(xm.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c17fa206-b537-4acf-bbe0-c20d15b2abb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18454, 3584])\n"
     ]
    }
   ],
   "source": [
    "CNN.gcnn.cpu()\n",
    "CNN.gcnn.eval()\n",
    "#CNN.gcnn.eval()\n",
    "with torch.no_grad():\n",
    "    xg = CNN.gcnn(train_data['gene'].seq)\n",
    "    #xg = CNN.gcnn(Xg)\n",
    "\n",
    "print(xg.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d86c330-c93d-40e1-9f51-f9e7ba754f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mcnn.stem.0.1.weight : torch.Size([128, 4, 3])\n",
      "1 mcnn.stem.0.1.bias : torch.Size([128])\n",
      "2 mcnn.stem.1.fn.0.weight : torch.Size([128])\n",
      "3 mcnn.stem.1.fn.0.bias : torch.Size([128])\n",
      "4 mcnn.stem.1.fn.2.1.weight : torch.Size([128, 128, 3])\n",
      "5 mcnn.stem.1.fn.2.1.bias : torch.Size([128])\n",
      "6 mcnn.transformer_encoder.layers.0.self_attn.in_proj_weight : torch.Size([384, 128])\n",
      "7 mcnn.transformer_encoder.layers.0.self_attn.in_proj_bias : torch.Size([384])\n",
      "8 mcnn.transformer_encoder.layers.0.self_attn.out_proj.weight : torch.Size([128, 128])\n",
      "9 mcnn.transformer_encoder.layers.0.self_attn.out_proj.bias : torch.Size([128])\n",
      "10 mcnn.transformer_encoder.layers.0.linear1.weight : torch.Size([256, 128])\n",
      "11 mcnn.transformer_encoder.layers.0.linear1.bias : torch.Size([256])\n",
      "12 mcnn.transformer_encoder.layers.0.linear2.weight : torch.Size([128, 256])\n",
      "13 mcnn.transformer_encoder.layers.0.linear2.bias : torch.Size([128])\n",
      "14 mcnn.transformer_encoder.layers.0.norm1.weight : torch.Size([128])\n",
      "15 mcnn.transformer_encoder.layers.0.norm1.bias : torch.Size([128])\n",
      "16 mcnn.transformer_encoder.layers.0.norm2.weight : torch.Size([128])\n",
      "17 mcnn.transformer_encoder.layers.0.norm2.bias : torch.Size([128])\n",
      "18 gcnn.stem1.0.conv1.0.weight : torch.Size([7, 15, 7, 1])\n",
      "19 gcnn.stem1.0.conv1.0.bias : torch.Size([7])\n",
      "20 gcnn.stem1.0.norm1.weight : torch.Size([7])\n",
      "21 gcnn.stem1.0.norm1.bias : torch.Size([7])\n",
      "22 gcnn.stem1.0.eca2.conv.weight : torch.Size([1, 1, 3])\n",
      "23 gcnn.stem1.0.conv2.0.weight : torch.Size([1, 7, 7, 1])\n",
      "24 gcnn.stem1.0.conv2.0.bias : torch.Size([1])\n",
      "25 gcnn.stem1.0.norm2.weight : torch.Size([1])\n",
      "26 gcnn.stem1.0.norm2.bias : torch.Size([1])\n",
      "27 gcnn.stem1.0.eca3.conv.weight : torch.Size([1, 1, 3])\n",
      "28 gcnn.stem2.0.1.weight : torch.Size([128, 4, 3])\n",
      "29 gcnn.stem2.0.1.bias : torch.Size([128])\n",
      "30 gcnn.stem2.1.fn.0.weight : torch.Size([128])\n",
      "31 gcnn.stem2.1.fn.0.bias : torch.Size([128])\n",
      "32 gcnn.stem2.1.fn.2.1.weight : torch.Size([128, 128, 3])\n",
      "33 gcnn.stem2.1.fn.2.1.bias : torch.Size([128])\n",
      "34 gcnn.stem3.0.fn.0.weight : torch.Size([128])\n",
      "35 gcnn.stem3.0.fn.0.bias : torch.Size([128])\n",
      "36 gcnn.stem3.0.fn.2.1.weight : torch.Size([128, 128, 3])\n",
      "37 gcnn.stem3.0.fn.2.1.bias : torch.Size([128])\n",
      "38 gcnn.transformer_encoder.layers.0.self_attn.in_proj_weight : torch.Size([384, 128])\n",
      "39 gcnn.transformer_encoder.layers.0.self_attn.in_proj_bias : torch.Size([384])\n",
      "40 gcnn.transformer_encoder.layers.0.self_attn.out_proj.weight : torch.Size([128, 128])\n",
      "41 gcnn.transformer_encoder.layers.0.self_attn.out_proj.bias : torch.Size([128])\n",
      "42 gcnn.transformer_encoder.layers.0.linear1.weight : torch.Size([256, 128])\n",
      "43 gcnn.transformer_encoder.layers.0.linear1.bias : torch.Size([256])\n",
      "44 gcnn.transformer_encoder.layers.0.linear2.weight : torch.Size([128, 256])\n",
      "45 gcnn.transformer_encoder.layers.0.linear2.bias : torch.Size([128])\n",
      "46 gcnn.transformer_encoder.layers.0.norm1.weight : torch.Size([128])\n",
      "47 gcnn.transformer_encoder.layers.0.norm1.bias : torch.Size([128])\n",
      "48 gcnn.transformer_encoder.layers.0.norm2.weight : torch.Size([128])\n",
      "49 gcnn.transformer_encoder.layers.0.norm2.bias : torch.Size([128])\n",
      "50 predict.lins.0.weight : torch.Size([1024, 3584])\n",
      "51 predict.lins.0.bias : torch.Size([1024])\n",
      "52 predict.bilin.weight : torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for name,parameters in CNN.named_parameters():\n",
    "    print(i,name,':',parameters.size())\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "472920ea-3e6c-4617-816a-0a7e9238f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/hnuResults/TranCNN_mgEmbedding.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([xm,xg], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbdb2852-1d72-4130-bfa5-64704d9c4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/TranCNN_mgEmbedding_usingmiRNAanchor.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([xm,xg], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a450d2c-c6ab-407b-baa9-ba2caa887b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
